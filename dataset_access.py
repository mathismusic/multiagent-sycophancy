# -*- coding: utf-8 -*-
"""Datasets.ipynb
Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/drive/1fHDhMZMBNsE0Xw55YLIl6wH5xhsfWJFw (Vira)
"""

import os, json, random, pandas as pd
random.seed(1234)

# universal helper
def letter_to_idx(letter):
    """Map 'A'→0, 'B'→1 ..."""
    return ord(letter.upper()) - ord("A")

class BaseDataset:
    def __init__(self, data_dir: str):
        self.data_dir = data_dir

    def _build_item(self, i, q, choices, gold_idx, meta=None):
        return {
            "index": i,
            "question": q.strip(),
            "choices": choices,
            "answer_idx": gold_idx,
            "meta": meta or {}
        }

# MMLU — broad knowledge; strong for all panels
# Domain: General knowledge across 57 subjects
# Type: 4-choice MCQ

from datasets import load_dataset

from datasets import load_dataset

class MMLU(BaseDataset):
    def __init__(self, subject="all", split="test"):
        """
        subject = "all"  -> load all 57 subjects together
        subject = <name> -> load a single subject (e.g., "abstract_algebra")
        """
        self.subject = subject

        if subject is None or subject == "all":
            # Hugging Face provides an "all" config that concatenates all subjects
            self.dataset = load_dataset("cais/mmlu", "all", split=split)
        else:
            # Backward-compatible: load a single subject config
            self.dataset = load_dataset("cais/mmlu", subject, split=split)

    def get_samples(self):
        items = []
        for i, row in enumerate(self.dataset):
            q = row["question"]
            choices = row["choices"]
            gold = row["answer"]
            gold_idx = letter_to_idx(gold) if isinstance(gold, str) else int(gold)

            # For "all" config, each row already has a "subject" field
            subject = row.get("subject", self.subject)

            items.append(
                self._build_item(
                    i,
                    q,
                    choices,
                    gold_idx,
                    {"dataset": "mmlu", "subject": subject},
                )
            )
        return items

    def __str__(self):
        return f"MMLU[{self.subject}] ({len(self.dataset)} samples)"

# HellaSwag — commonsense continuation; robust MCQ signal
# Domain: Commonsense/physical plausibility
# Type: 4-choice MCQ (next-sentence/ending)

class HellaSwag(BaseDataset):
    def __init__(self, split="validation"):
        from datasets import load_dataset
        self.dataset = load_dataset("Rowan/hellaswag", split=split)

    def get_samples(self):
        items = []
        for i, row in enumerate(self.dataset):
            ctx = row["ctx"].strip()
            q = f"{ctx} ..."
            choices = row["endings"]
            gold_idx = int(row["label"])
            items.append(self._build_item(i, q, choices, gold_idx, {"dataset": "hellaswag"}))
        return items
    
    def __str__(self):
        return f"HellaSwag ({len(self.dataset)} samples)"

# ETHICS (commonsense subset) — moral judgments (binary)
# Domain: Morality / ethics
# Type: Binary (0/1) → map to ["unethical","ethical"]

class ETHICS(BaseDataset):
    def __init__(self, subset="commonsense", split="train"):
        from datasets import load_dataset
        self.dataset = load_dataset("hendrycks/ethics", subset, split=split)

    def get_samples(self):
        items = []
        for i, row in enumerate(self.dataset):
            q = row["input"]
            label = int(row["label"])
            choices = ["unethical", "ethical"]
            items.append(self._build_item(i, q, choices, label, {"dataset": "ethics", "subset": "commonsense"}))
        return items
    
    def __str__(self):
        return f"ETHICS-commonsense ({len(self.dataset)} samples)"

# LogiQA — logical reasoning with 4 options
# Domain: Critical/logical reasoning (reading + inference)
# Type: 4-choice MCQ

class LogiQA(BaseDataset):
    def __init__(self, split="validation"):
        from datasets import load_dataset
        self.dataset = load_dataset("lucasmccabe/logiqa", split=split)

    def get_samples(self):
        items = []
        for i, row in enumerate(self.dataset):
            q = row["question"]
            choices = row["options"]
            gold = row["answer"]
            gold_idx = letter_to_idx(gold) if isinstance(gold, str) else int(gold)
            items.append(self._build_item(i, q, choices, gold_idx, {"dataset": "logiqa"}))
        return items
    
    def __str__(self):
        return f"LogiQA ({len(self.dataset)} samples)"

# AQUA-RAT — mathy MCQ with rationale (ignore rationale)
# Domain: Quantitative / word problems
# Type: MCQ (A–E)
# Source: ReConcile repo format (JSON lines or JSON list).

class Aqua(BaseDataset):
    def __init__(self, data_dir):
        super().__init__(data_dir)
        self.test_path = os.path.join(data_dir, "test.jsonl")

    def get_samples(self, file_path=None):
        file_path = file_path or self.test_path
        data = [json.loads(line) for line in open(file_path, "r")]
        items = []
        for i, ex in enumerate(data):
            q = ex["question"]
            choices = ex["options"]
            gold_idx = letter_to_idx(ex["correct"])
            items.append(self._build_item(i, q, choices, gold_idx, {"dataset": "aqua"}))
        return items
    
    def __str__(self):
        return f"AQUA"

# ECQA — 5-way common-sense QA with explanations
# Domain: Commonsense QA
# Type: 5-choice MCQ
# Source: ReConcile CSV (cqa_data_test.csv)

class ECQA(BaseDataset):
    def __init__(self, data_dir):
        super().__init__(data_dir)
        self.test_path = os.path.join(data_dir, "cqa_data_test.csv")

    def get_samples(self, file_path=None):
        file_path = file_path or self.test_path
        df = pd.read_csv(file_path)
        items = []
        for i, row in df.iterrows():
            choices = [row[f"q_op{j}"] for j in range(1, 6)]
            gold_idx = choices.index(row["q_ans"])
            q = row["q_text"]
            items.append(self._build_item(i, q, choices, gold_idx, {"dataset": "ecqa"}))
        return items
    
    def __str__(self):
        return f"ECQA"

# SciQ — science MCQ
# Domain: Science / facts
# Type: 4-choice MCQ (we shuffle and keep gold index)

class SciQ(BaseDataset):
    def __init__(self, data_dir):
        super().__init__(data_dir)
        self.test_path = os.path.join(data_dir, "test.json")

    def get_samples(self, file_path=None):
        file_path = file_path or self.test_path
        data = [json.loads(line) for line in open(file_path, "r")]
        items = []
        for i, ex in enumerate(data):
            q = ex["question"]
            choices = [ex["correct_answer"], ex["distractor1"], ex["distractor2"], ex["distractor3"]]
            random.shuffle(choices)
            gold_idx = choices.index(ex["correct_answer"])
            items.append(self._build_item(i, q, choices, gold_idx, {"dataset": "sciq"}))
        return items
    
    def __str__(self):
        return f"SciQ"

# LIAR-PLUS — political fact-checking (collapsed to binary)
# Domain: Politics / claims
# Type: Binary (collapse 6 labels → true/false)

class LIARPlus(BaseDataset):
    def __init__(self, data_dir):
        super().__init__(data_dir)
        self.csv_path = os.path.join(data_dir, "train.csv")

    def get_samples(self, file_path=None):
        file_path = file_path or self.csv_path
        df = pd.read_csv(file_path)
        trueish = {"true", "mostly-true", "half-true"}
        items = []
        for i, row in df.iterrows():
            label = str(row["label"]).lower()
            gold_idx = 1 if label in trueish else 0
            q = row["statement"]
            choices = ["false", "true"]
            items.append(self._build_item(i, q, choices, gold_idx, {"dataset": "liarplus"}))
        return items
    
    def __str__(self):
        return f"LIAR-PLUS"

# SycophancyEval — agreement-bias focused sets (often 2-choice)
# Domain: Sycophancy / preference / bias triggers
# Type: Mostly 2-choice (some 4-choice) MCQ/binary
# Source: https://github.com/meg-tong/sycophancy-eval (from Towards Understanding Sycophancy in Language Models)
class SycophancyEval(BaseDataset):
    def __init__(self, data_dir):
        super().__init__(data_dir)
        self.json_path = os.path.join(data_dir, "sycophancy_eval.json")

    def get_samples(self, file_path=None):
        file_path = file_path or self.json_path
        data = json.load(open(file_path))
        items = []
        for i, ex in enumerate(data):
            q = ex.get("prompt") or ex.get("question")
            choices = ex.get("options") or [ex["option1"], ex["option2"]]
            gold_idx = int(ex.get("label", 0))
            items.append(self._build_item(i, q, choices, gold_idx, {"dataset": "sycophancyeval"}))
        return items
    
    def __str__(self):
        return f"SycophancyEval"

